{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### часть 1\n",
    "* реализация Gradient Descent для модели линейной регрессии\n",
    "\n",
    "### часть 2\n",
    "* реализация Back Propagation для MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor:\n",
    "    def __init__( \n",
    "        self, \n",
    "        hidden layer_sizes=(100,), \n",
    "        learning_rate=0.001, \n",
    "        max_iter=10,\n",
    "    ):\n",
    "        pass\n",
    "    def train(self, x, y): \n",
    "        pass\n",
    "    def predict(self, X): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### часть 1\n",
    "* реализация Gradient Descent для модели линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.theta = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), X]  # Добавляем столбец единиц для учета bias (свободного члена)\n",
    "        self.theta = np.zeros(n + 1)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            gradients = (2/m) * X.T.dot(X.dot(self.theta) - y)\n",
    "            self.theta -= self.learning_rate * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]  # Добавляем столбец единиц для учета bias\n",
    "        return X.dot(self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### часть 2\n",
    "* реализация Back Propagation для MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLPRegressor:\n",
    "    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.001, max_iter=1000):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "    def _initialize_weights(self, input_size, output_size):\n",
    "        layer_sizes = [input_size] + list(self.hidden_layer_sizes) + [output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    \n",
    "    def _relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def _relu_derivative(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            Z = activations[-1].dot(self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(Z)\n",
    "            A = self._relu(Z) if i < len(self.weights) - 1 else Z  # Применение ReLU для скрытых слоев и линейной активации для выхода\n",
    "            activations.append(A)\n",
    "        \n",
    "        return activations, pre_activations\n",
    "    \n",
    "    def _back_propagation(self, activations, pre_activations, y):\n",
    "        m = y.shape[0]\n",
    "        grads_W = [None] * len(self.weights)\n",
    "        grads_b = [None] * len(self.biases)\n",
    "        \n",
    "        # Вычисление ошибки на выходе\n",
    "        dZ = activations[-1] - y\n",
    "        grads_W[-1] = activations[-2].T.dot(dZ) / m\n",
    "        grads_b[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Распространение ошибки назад по сети\n",
    "        for i in reversed(range(len(grads_W) - 1)):\n",
    "            dA = dZ.dot(self.weights[i+1].T)\n",
    "            dZ = dA * self._relu_derivative(pre_activations[i])\n",
    "            grads_W[i] = activations[i].T.dot(dZ) / m\n",
    "            grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return grads_W, grads_b\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        input_size = X.shape[1]\n",
    "        output_size = y.shape[1] if len(y.shape) > 1 else 1\n",
    "        self._initialize_weights(input_size, output_size)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Прямое распространение\n",
    "            activations, pre_activations = self._forward_propagation(X)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            grads_W, grads_b = self._back_propagation(activations, pre_activations, y)\n",
    "            \n",
    "            # Обновление весов и смещений\n",
    "            for j in range(len(self.weights)):\n",
    "                self.weights[j] -= self.learning_rate * grads_W[j]\n",
    "                self.biases[j] -= self.learning_rate * grads_b[j]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        activations, _ = self._forward_propagation(X)\n",
    "        return activations[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest-pandas",
   "language": "python",
   "name": "latest-pandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
