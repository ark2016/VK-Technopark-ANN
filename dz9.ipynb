{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HmwpqzwnMyE"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/KuzmaKhrabrov/character-tokenizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkVexA2nnRQ5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5FaCG9ajnS_G"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "sys.path.append(\"/content/character-tokenizer\")\n",
    "from charactertokenizer import CharacterTokenizer\n",
    "\n",
    "chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
    "model_max_length = 64\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I5FSPMOSncpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example = \"Привет\"\n",
    "tokens = tokenizer(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQkp36rEoScR"
   },
   "source": [
    "Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n",
    "\n",
    "1. Напишите класс для Dataset/Dataloder и разбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n",
    "2. Попробуйте обучить одну или несколько из моделей: Bert, Albert, Deberta. Посчитайте метрику Accuracy на train и test. (1 балл). При преодолении порога в Accuracy на test 0.8: (+1 балл), 0.85: (+2 балла), 0.89: (+3 балла).\n",
    "Пример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForTokenClassification, BertConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "sys.path.append(\"/content/character-tokenizer\")\n",
    "from charactertokenizer import CharacterTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-де</td>\n",
       "      <td>-д^е</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-ка</td>\n",
       "      <td>-к^а</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-либо</td>\n",
       "      <td>-л^ибо</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-нибудь</td>\n",
       "      <td>-ниб^удь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-с</td>\n",
       "      <td>-с</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word    target\n",
       "0      -де      -д^е\n",
       "1      -ка      -к^а\n",
       "2    -либо    -л^ибо\n",
       "3  -нибудь  -ниб^удь\n",
       "4       -с        -с"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_accents.tsv\", sep=\"\\t\", header=None)\n",
    "df = df.rename({0: \"word\", 1:\"target\"}, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_for_text(target, get_indexes=True):\n",
    "    class2id = {\n",
    "        \"O\": 0,\n",
    "        \"X\": 1\n",
    "    }\n",
    "    result = []\n",
    "    last = False\n",
    "    for char in target:\n",
    "        if last:\n",
    "            last = False\n",
    "        elif char == \"^\":\n",
    "            result.append(\"X\")\n",
    "            last = True\n",
    "        else:\n",
    "            result.append(\"O\")\n",
    "    if get_indexes:\n",
    "        result = [class2id[value] for value in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StressDataset(Dataset):\n",
    "    def __init__(self, df, MAX_LENGTH=64):\n",
    "        super(StressDataset, self).__init__()\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        self.df = df\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.df.iloc[idx]\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            line[\"word\"],                     \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = self.MAX_LENGTH,           \n",
    "                            padding='max_length',\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   \n",
    "                            return_tensors = 'pt',     \n",
    "                            return_special_tokens_mask=True,\n",
    "                    )\n",
    "        lebels = get_labels_for_text(line[\"target\"])\n",
    "        encoded_dict['input_ids'] = encoded_dict['input_ids'].squeeze()\n",
    "        encoded_dict['special_tokens_mask'] = encoded_dict['special_tokens_mask'].squeeze()\n",
    "        encoded_dict['attention_mask'] = encoded_dict['attention_mask'].squeeze()\n",
    "        lebels = torch.tensor([-100] + lebels + [-100] * (encoded_dict['input_ids'].shape[-1] - (1 + len(lebels)))).long()\n",
    "        return (encoded_dict, lebels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = StressDataset(train_data)\n",
    "dataset_val = StressDataset(val_data)\n",
    "train_loader = DataLoader(dataset_train, batch_size=784, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=784, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    lst_losses = []\n",
    "    correct_elements = 0\n",
    "    total_elements = 0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader)):\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "        outputs = model(\n",
    "            input_ids = inputs['input_ids'],\n",
    "            attention_mask =inputs['attention_mask'],\n",
    "            labels = labels,\n",
    "        )\n",
    "        loss, scores = outputs.loss, outputs.logits\n",
    "        optimizer.zero_grad()\n",
    "        lst_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_loss = np.array(lst_losses).mean()\n",
    "    print(f\"Train: epoch {epoch} | loss = {avg_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(epoch, model, val_loader, optimizer):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        lst_losses = []\n",
    "        correct_elements = 0\n",
    "        total_elements = 0\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(val_loader)):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(\n",
    "                input_ids = inputs['input_ids'],\n",
    "                attention_mask =inputs['attention_mask'],\n",
    "                labels = labels,\n",
    "            )\n",
    "            loss, scores = outputs.loss, outputs.logits\n",
    "            lst_losses.append(loss.item())\n",
    "\n",
    "            for ind_elem in range(inputs['input_ids'].shape[0]):\n",
    "                x1 = torch.masked_select(scores[ind_elem, :, :].argmax(dim=-1), inputs['special_tokens_mask'][ind_elem] == 0)\n",
    "                x2 = torch.masked_select(labels[ind_elem], labels[ind_elem] != -100)\n",
    "                if torch.equal(x1, x2):\n",
    "                    correct_elements += 1\n",
    "                total_elements += 1\n",
    "\n",
    "        avg_loss = np.array(lst_losses).mean()\n",
    "\n",
    "        print(f\"Val: epoch {epoch} | loss = {avg_loss} | accuracy = {(correct_elements / total_elements)* 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=256,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(73, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:3\" \n",
    "model = BertForTokenClassification(configuration,)\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 2)\n",
    "configuration = model.config\n",
    "# model= nn.DataParallel(model,device_ids = [3,4, 5,7])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=392, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(dataset_val, batch_size=392, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_steps = 1000\n",
    "num_training_steps= NUM_EPOCHS * len(train_loader)\n",
    "num_cycles = 0.5\n",
    "last_epoch = -1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_cycles=num_cycles,\n",
    "    last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2144it [02:08, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 1 | loss = 0.09263319548205542 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [03:43,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: epoch 1 | loss = 0.07772094627439197 | accuracy = 79.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:07, 16.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 2 | loss = 0.08020266937786963 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:18, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 3 | loss = 0.07020231828253383 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [03:49,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: epoch 3 | loss = 0.056171288512257944 | accuracy = 85.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:16, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 4 | loss = 0.06239392812912867 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:17, 15.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 5 | loss = 0.055654147693734446 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [03:43,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: epoch 5 | loss = 0.04323482880309752 | accuracy = 89.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:14, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 6 | loss = 0.04997781533817413 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:20, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 7 | loss = 0.0456450547963909 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [03:47,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: epoch 7 | loss = 0.03651020029531931 | accuracy = 91.10000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:14, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 8 | loss = 0.0427387932345007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [02:07, 16.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 9 | loss = 0.04155977822166047 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2144it [03:50,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: epoch 9 | loss = 0.03489579213696268 | accuracy = 91.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train_one_epoch(epoch, model, train_loader, optimizer, scheduler)\n",
    "    if epoch % 2 == 1:\n",
    "        validate_one_epoch(epoch, model, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### итоги\n",
    "Луший скор __91,53__\n",
    "\n",
    "Выбраная архитектура - BERT (BertForTokenClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "latest-pandas",
   "language": "python",
   "name": "latest-pandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
